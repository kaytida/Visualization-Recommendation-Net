{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('all_data.pkl', 'rb') as f:\n",
    "    all_data = pickle.load(f)\n",
    "\n",
    "# shuffle the data\n",
    "random.shuffle(all_data)\n",
    "\n",
    "# split the data into train and test\n",
    "train_data, test_data = train_test_split(all_data, test_size=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some encodings\n",
    "\n",
    "with open('./kdd21-MLVis-main/data/1k/tmp/meta_variable_mapping.pkl', 'rb') as f:\n",
    "    meta_variable_mapping = pickle.load(f)\n",
    "\n",
    "with open('./kdd21-MLVis-main/data/1k/tmp/wide-and-deep-config2id.pkl','rb') as f:\n",
    "    config2id = pickle.load(f)\n",
    "\n",
    "with open('./kdd21-MLVis-main/data/1k/tmp/wide-and-deep-dataset2id.pkl','rb') as f:\n",
    "    dataset2id = pickle.load(f)\n",
    "\n",
    "# one_hot_encoding of 60 numerical features\n",
    "one_hot_c = np.eye(60)\n",
    "# # print(config2id.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feature extraction\n",
    "def get_sparse_features(feature,n):\n",
    "    # normalize the feature to [0,1], by substracting minimum and dividing by range\n",
    "    # print(type(feature[0]))\n",
    "    # convert the feature into a a numpy array of numbers\n",
    "    feature = np.array(feature)\n",
    "    # print(type(feature[0]))\n",
    "\n",
    "    feature = (feature - feature.min())/(feature.max() - feature.min())\n",
    "\n",
    "    # # print(feature)\n",
    "    # divide the range[0,1] into n bins\n",
    "    bins = np.linspace(0,1,n+1)\n",
    "    # print(bins)\n",
    "    # get the index of the bin that each feature belongs to\n",
    "    feature = np.digitize(feature,bins)\n",
    "    # print(feature)\n",
    "    # convert the index to a sparse feature\n",
    "    feature = np.eye(n+2)[feature]\n",
    "    # make it to a 1D array\n",
    "    # print(feature[0][0])\n",
    "    feature = feature.reshape(-1)\n",
    "    return feature\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_s1(n,b=4):\n",
    "    # return an array of 4 vectors of length n\n",
    "    # each vector is a sparse feature of a numerical feature\n",
    "    # and has 0 and 1 as values with 0.8 probability and 0.2 probability respectively\n",
    "    s1 = np.random.choice([0,1],size=(b,n),p=[0.8,0.2])\n",
    "    return torch.tensor(s1).double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "\n",
    "s1_len = 1006\n",
    "dc_len = 10\n",
    "dx_len = 2*1006\n",
    "sc_len = 60\n",
    "sx_len = 2*1006*5\n",
    "\n",
    "wide_len = sx_len + sc_len + s1_len\n",
    "deep_len = dc_len + dx_len\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # self.wide_len = wide_len\n",
    "        # self.deep_len = deep_len\n",
    "        \n",
    "        # matrix to extract dense features\n",
    "        self.dense_c = nn.Linear(sc_len, dc_len).double()\n",
    "\n",
    "        # wide model parameters\n",
    "        self.wide_w = nn.Linear(wide_len,100).double()\n",
    "        self.wide_b = nn.Parameter(torch.zeros(100)).double()\n",
    "\n",
    "        # deep model parameters\n",
    "        # deep model is a 3 layer MLP, with 2*1006+dc_len neurons in the first layer, 1006 neurons in the second layer, and 1 neuron in the third layer, \n",
    "        # with relu activation function\n",
    "        self.deep1_w = nn.Linear(deep_len,500).double()\n",
    "        self.deep1_b = nn.Parameter(torch.zeros(500)).double()\n",
    "        self.deep2_w = nn.Linear(500,100).double()\n",
    "        self.deep2_b = nn.Parameter(torch.zeros(100)).double()\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # final score parameters\n",
    "        self.final_w = nn.Linear(100,1).double()\n",
    "        self.final_d = nn.Linear(100,1).double()\n",
    "        self.final_b = nn.Parameter(torch.zeros(1)).double()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "                \n",
    "\n",
    "\n",
    "    def forward(self,dense,sparse,config):\n",
    "\n",
    "        dx = dense\n",
    "        sx = sparse\n",
    "        \n",
    "        sc = np.eye(60)[config]\n",
    "        dx = torch.tensor(dx)\n",
    "        # sc = np.eye(60)[sc]\n",
    "        sc = torch.tensor(sc)\n",
    "        sx = torch.tensor(sx)\n",
    "\n",
    "        dc = self.dense_c(sc)\n",
    "        \n",
    "        s1 = generate_s1(s1_len)\n",
    "        # concatanete dc and dx to get d\n",
    "        try:\n",
    "            d = torch.cat((dc,dx),1)\n",
    "            s = torch.cat((sx,sc),1)\n",
    "            \n",
    "\n",
    "        except:\n",
    "            # print(dc.shape)\n",
    "            # print(dx.shape)\n",
    "            d = torch.cat((dc,dx))\n",
    "            s = torch.cat((sx,sc))\n",
    "            s1 = generate_s1(s1_len,1)\n",
    "            s1 = s1.reshape(-1)\n",
    "            # print(dc)\n",
    "            # print(dx)\n",
    "        # try:\n",
    "        # s1 = torch.tens(s1)\n",
    "        try:    \n",
    "            wide_s = torch.cat((s,s1),1)\n",
    "        except:\n",
    "            # print(s.shape)\n",
    "            # print(s1.shape)\n",
    "            wide_s = torch.cat((s,s1))\n",
    "        wide_s = torch.tensor(wide_s)\n",
    "        \n",
    "        wide = self.wide_w(wide_s) + self.wide_b\n",
    "        \n",
    "        d = torch.tensor(d)\n",
    "        \n",
    "    \n",
    "        # print(\"error at deep encoding\")\n",
    "    \n",
    "    \n",
    "        deep = self.deep1_w(d) + self.deep1_b\n",
    "        deep = self.relu(deep)\n",
    "        deep = self.deep2_w(deep) + self.deep2_b\n",
    "        deep = self.relu(deep)\n",
    "        \n",
    "        wide = torch.tensor(wide)\n",
    "        deep = torch.tensor(deep)\n",
    "        final = self.final_w(wide) + self.final_d(deep) + self.final_b\n",
    "        \n",
    "        final = torch.tensor(final)\n",
    "        final = self.sigmoid(final)\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15910/1655398477.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_data = np.array(train_data)\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "def get_dense_x(data):\n",
    "\n",
    "    if(len(data) == 5):\n",
    "        var1 = data[1]\n",
    "        var2 = data[2]\n",
    "    if(len(data) == 4):\n",
    "        var1 = data[1]\n",
    "    \n",
    "    d1 = meta_variable_mapping[var1]\n",
    "    if(len(data) == 5):\n",
    "        d2 = meta_variable_mapping[var2]\n",
    "        \n",
    "        # concatenate d1 and d2\n",
    "        dx = torch.cat((d1,d2))\n",
    "    else:\n",
    "        dx = d1\n",
    "    \n",
    "    # concatenate d1 and d2 to get dx\n",
    "    # dx = torch.cat((d1,d2),1)\n",
    "\n",
    "    return dx\n",
    "\n",
    "def get_sparse_c(data):\n",
    "    config_id = config2id[data[-2]]\n",
    "    # get the one hot encoding of the config id\n",
    "    sc = one_hot_c[config_id]\n",
    "\n",
    "    return sc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8531\n"
     ]
    }
   ],
   "source": [
    "modified_data = []\n",
    "sparse_vactors = []\n",
    "print(len(all_data))\n",
    "for data in all_data:\n",
    "\n",
    "    dataset = data[0]\n",
    "    try:\n",
    "        dataset = dataset2id[dataset]\n",
    "        # print(dataset)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    var1 = data[1]\n",
    "    try:    \n",
    "        var1 = meta_variable_mapping[var1]\n",
    "        dense = var1\n",
    "    except:\n",
    "        continue\n",
    "    if(len(data) == 5):\n",
    "        var2 = data[2]\n",
    "        try:\n",
    "            var2 = meta_variable_mapping[var2]\n",
    "            dense = np.concatenate((var1,var2))\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    if(len(data) == 4):\n",
    "        var2 = torch.zeros(1006)\n",
    "        try:\n",
    "            dense = np.concatenate((var1,var2))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    config = data[-2]\n",
    "    try:\n",
    "        config = config2id[config]\n",
    "    except:\n",
    "        continue\n",
    "    try:\n",
    "        label = int(data[-1])\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # dense = np.concatenate((var1,var2))\n",
    "    var1 = get_sparse_features(var1,3)\n",
    "    if(len(data) == 5):\n",
    "        var2 = get_sparse_features(var2,3)\n",
    "        var1 = np.concatenate((var1,var2))\n",
    "    if(len(data) == 4):\n",
    "        var2 = np.zeros(1006*5)\n",
    "        var1 = np.concatenate((var1,var2))\n",
    "\n",
    "    # make a list of the modified data\n",
    "    sparse = var1\n",
    "    modified_data.append([dataset,dense,sparse,config,label])\n",
    "\n",
    "import torch.nn as nn\n",
    "# import Dataloder utility from pytorch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4\n",
    "epochs = 3\n",
    "\n",
    "train_data , test_data = train_test_split(modified_data,test_size=0.2,random_state=42)\n",
    "\n",
    "# divide train into batches of 4 using dataloader\n",
    "train_batches = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# divide test into batches of 4 using dataloader\n",
    "# test_test = DataLoader(test_data,batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = Model()\n",
    "    # define loss function\n",
    "    loss_fn = nn.BCELoss()\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    # train the model\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_batches:\n",
    "            \n",
    "            if(len(batch) != 4):\n",
    "                continue\n",
    "            # make gradients zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            dense = torch.tensor(batch[1])\n",
    "            sparse = torch.tensor(batch[2])\n",
    "            config = (batch[3])\n",
    "            label = batch[4]\n",
    "\n",
    "            \n",
    "            \n",
    "            pred = model(dense,sparse,config)\n",
    "            try:\n",
    "                loss = loss_fn(pred,label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            except:\n",
    "                continue\n",
    "        try:    \n",
    "            print(\"epoch: \",epoch,\" loss: \",loss.item())\n",
    "        except:\n",
    "            print()\n",
    "    return model\n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec  2 01:25:02 2022\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15910/128232816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dx = torch.tensor(dx)\n",
      "/tmp/ipykernel_15910/128232816.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sx = torch.tensor(sx)\n",
      "/tmp/ipykernel_15910/128232816.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  wide_s = torch.tensor(wide_s)\n",
      "/tmp/ipykernel_15910/128232816.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  d = torch.tensor(d)\n",
      "/tmp/ipykernel_15910/128232816.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  wide = torch.tensor(wide)\n",
      "/tmp/ipykernel_15910/128232816.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  deep = torch.tensor(deep)\n",
      "/tmp/ipykernel_15910/128232816.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  final = torch.tensor(final)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_0:  0.8871951219512195\n",
      "score_1:  0.9691629955947136\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "import time\n",
    "print(time.ctime())\n",
    "model = train()\n",
    "\n",
    "# separate data based on label 0 and 1\n",
    "def separate_data(data):\n",
    "    data_0 = []\n",
    "    data_1 = []\n",
    "    for d in data:\n",
    "        if(d[-1] == 0):\n",
    "            data_0.append(d)\n",
    "        else:\n",
    "            data_1.append(d)\n",
    "    return data_0,data_1\n",
    "\n",
    "# get the data with label 0 and 1\n",
    "data_0,data_1 = separate_data(test_data)\n",
    "\n",
    "# predict the labels\n",
    "\n",
    "score = 0\n",
    "\n",
    "for data in data_0:\n",
    "    dense = torch.tensor(data[1])\n",
    "    sparse = torch.tensor(data[2])\n",
    "    config = data[3]\n",
    "    label = data[4]\n",
    "    pred = model(dense,sparse,config)\n",
    "    pred = pred.item()\n",
    "    if(pred < 0.5):\n",
    "        score += 1\n",
    "\n",
    "print(\"score_0: \",score/len(data_0))\n",
    "\n",
    "score = 0\n",
    "for data in data_1:\n",
    "    dense = torch.tensor(data[1])\n",
    "    sparse = torch.tensor(data[2])\n",
    "    config = data[3]\n",
    "    label = data[4]\n",
    "    pred = model(dense,sparse,config)\n",
    "    pred = pred.item()\n",
    "    if(pred < 0.5):\n",
    "        score += 1\n",
    "\n",
    "print(\"score_1: \",score/len(data_1))\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (dense_c): Linear(in_features=60, out_features=10, bias=True)\n",
      "  (wide_w): Linear(in_features=11126, out_features=100, bias=True)\n",
      "  (deep1_w): Linear(in_features=2022, out_features=500, bias=True)\n",
      "  (deep2_w): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (final_w): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (final_d): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Model(\n",
      "  (dense_c): Linear(in_features=60, out_features=10, bias=True)\n",
      "  (wide_w): Linear(in_features=11126, out_features=100, bias=True)\n",
      "  (deep1_w): Linear(in_features=2022, out_features=500, bias=True)\n",
      "  (deep2_w): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (final_w): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (final_d): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.0526,  0.0421,  0.0063, -0.0086, -0.0531, -0.0808, -0.0919, -0.1165,\n",
      "          0.0533,  0.0793, -0.0921, -0.0243,  0.0406,  0.1144,  0.0171,  0.1153,\n",
      "          0.0203, -0.0867, -0.0004, -0.1071,  0.1053,  0.1040, -0.0268, -0.0971,\n",
      "         -0.0051, -0.0909, -0.0491, -0.0377,  0.1243,  0.0254,  0.1065,  0.0583,\n",
      "          0.0404, -0.0355, -0.0185, -0.0905, -0.0278,  0.0033,  0.0504, -0.0711,\n",
      "          0.0648,  0.0545, -0.0936, -0.0834, -0.0260, -0.0548, -0.0399, -0.0324,\n",
      "         -0.0006,  0.0851,  0.1093,  0.1032,  0.0240, -0.0566, -0.0425, -0.1189,\n",
      "         -0.1232,  0.0952,  0.0735, -0.0499],\n",
      "        [ 0.1164,  0.0828,  0.1079,  0.0492, -0.0825, -0.0802, -0.0491,  0.1133,\n",
      "         -0.0093,  0.0832,  0.0922,  0.0559,  0.0737,  0.1083,  0.0034,  0.0015,\n",
      "         -0.0548, -0.1041,  0.0112, -0.0919,  0.1044, -0.0692,  0.0220, -0.0868,\n",
      "          0.0248,  0.1119, -0.0129,  0.0895,  0.0693,  0.0828, -0.0917, -0.0771,\n",
      "         -0.0221, -0.0157, -0.0814, -0.0863,  0.0794, -0.1174, -0.0491,  0.0977,\n",
      "         -0.0019,  0.0234,  0.1118,  0.0866, -0.0009, -0.0115,  0.0093,  0.0656,\n",
      "         -0.0580, -0.1061,  0.0476, -0.1151,  0.0633,  0.1107, -0.0507, -0.0927,\n",
      "         -0.1202,  0.0475,  0.0800, -0.0056],\n",
      "        [-0.1143, -0.0866, -0.0148, -0.1241,  0.0863,  0.0450, -0.0295,  0.0636,\n",
      "          0.1023,  0.0024,  0.0157, -0.0237, -0.0882,  0.1105, -0.1135, -0.0055,\n",
      "         -0.0147,  0.0279, -0.0451,  0.0177,  0.0974,  0.1048, -0.1050, -0.0751,\n",
      "          0.0803, -0.0915,  0.0690, -0.0561, -0.1119,  0.0375, -0.1212,  0.0101,\n",
      "          0.0902,  0.0826,  0.1221,  0.0682,  0.1142, -0.1144, -0.0271, -0.1104,\n",
      "         -0.1025,  0.0935,  0.0073,  0.0381,  0.0347,  0.0103,  0.0079,  0.1165,\n",
      "          0.0118,  0.0243, -0.0119, -0.0454,  0.1227,  0.0992, -0.0296, -0.0866,\n",
      "          0.0911,  0.0050, -0.0405, -0.0213],\n",
      "        [ 0.0659, -0.0541, -0.1026,  0.0250, -0.0062,  0.0292, -0.1141,  0.1143,\n",
      "          0.0233,  0.1048, -0.0420, -0.0132, -0.0383,  0.1240,  0.0114, -0.0136,\n",
      "          0.0807, -0.0899, -0.0991,  0.1208,  0.1137,  0.0349,  0.0057,  0.0015,\n",
      "          0.0872,  0.0813, -0.1221, -0.0074, -0.0518,  0.0705,  0.0394,  0.0035,\n",
      "         -0.1137, -0.0955,  0.0930, -0.1011, -0.1287, -0.0450,  0.0665,  0.0730,\n",
      "          0.1203,  0.0624, -0.0101,  0.0564, -0.0554, -0.1156, -0.1058,  0.0769,\n",
      "         -0.0616, -0.1165, -0.0532, -0.0184, -0.0388,  0.1276,  0.0078, -0.0561,\n",
      "          0.1074,  0.0511,  0.0540, -0.0321],\n",
      "        [-0.1109, -0.0559, -0.1109, -0.0787, -0.1275,  0.0021, -0.0356,  0.0208,\n",
      "         -0.0055, -0.1031, -0.0672,  0.1027,  0.0978, -0.1091, -0.0832,  0.0726,\n",
      "         -0.0311, -0.0807, -0.1071, -0.0098, -0.0500,  0.1055,  0.0737, -0.0599,\n",
      "          0.0701, -0.0953,  0.0834,  0.0257, -0.0528,  0.0658, -0.0297, -0.0863,\n",
      "          0.0353, -0.0089,  0.0452, -0.0811, -0.0369, -0.0378, -0.0611, -0.1268,\n",
      "          0.0310,  0.1263,  0.0738, -0.0860, -0.0795,  0.1216, -0.0819, -0.0055,\n",
      "         -0.0745,  0.1075,  0.1251,  0.1238,  0.0150,  0.0594, -0.0985, -0.1254,\n",
      "          0.0968, -0.0935,  0.1004,  0.0187],\n",
      "        [-0.0814, -0.0359, -0.1200,  0.0861, -0.0523,  0.0470,  0.0797, -0.0590,\n",
      "         -0.0864,  0.0515,  0.0189,  0.0793, -0.0224,  0.0612, -0.0461,  0.0807,\n",
      "         -0.0657, -0.1109, -0.0096, -0.0847,  0.1064,  0.0486, -0.0650, -0.0836,\n",
      "         -0.0391,  0.0975, -0.1057,  0.0665,  0.1217,  0.0844,  0.0693, -0.1059,\n",
      "          0.0197,  0.0108,  0.0603, -0.1219,  0.0523,  0.0514,  0.1195,  0.0902,\n",
      "          0.0308,  0.0481, -0.0399,  0.1089, -0.0321,  0.0503, -0.1068, -0.0428,\n",
      "          0.0400, -0.0675, -0.0874,  0.1202, -0.0193, -0.1225,  0.0717,  0.0024,\n",
      "          0.0096,  0.0935,  0.1279, -0.0255],\n",
      "        [ 0.0176,  0.0121,  0.1184,  0.0556,  0.0946,  0.0359,  0.1168,  0.1128,\n",
      "          0.0590, -0.0904,  0.1093, -0.0069, -0.0161, -0.0686,  0.0130, -0.0406,\n",
      "          0.0869, -0.0617,  0.0441, -0.0473,  0.0789, -0.0175, -0.0671,  0.1201,\n",
      "         -0.1188,  0.0923, -0.0967, -0.0268,  0.1111, -0.1234, -0.0146,  0.1264,\n",
      "          0.0856,  0.0440,  0.0171,  0.0116,  0.0316,  0.0145,  0.1152,  0.0382,\n",
      "          0.0976,  0.1096, -0.0015, -0.0193, -0.0916,  0.0092,  0.0803,  0.1060,\n",
      "          0.0614, -0.0469,  0.0675,  0.0016,  0.0524, -0.0741, -0.0529, -0.0965,\n",
      "          0.0576, -0.0815,  0.0852,  0.1111],\n",
      "        [-0.0742, -0.0476, -0.0461,  0.1059, -0.0482, -0.0853, -0.1021, -0.1078,\n",
      "         -0.0429,  0.0448, -0.0170,  0.0603,  0.0717,  0.0023,  0.0710, -0.0440,\n",
      "          0.0523,  0.0602, -0.0245,  0.1248, -0.0285,  0.0278, -0.1198,  0.0869,\n",
      "          0.1039, -0.0732,  0.0547,  0.0962, -0.1170,  0.0165, -0.1179,  0.0157,\n",
      "         -0.1265, -0.0485,  0.0810, -0.1076, -0.1147,  0.0904, -0.0197, -0.0961,\n",
      "         -0.0963,  0.1111,  0.0925, -0.0072,  0.1142, -0.1211,  0.0470, -0.0736,\n",
      "         -0.0189,  0.0033,  0.0717, -0.0149,  0.0308,  0.0195, -0.0548,  0.0383,\n",
      "          0.0829, -0.0074,  0.0603,  0.0878],\n",
      "        [ 0.0277, -0.0424, -0.0072, -0.0030,  0.0861,  0.0992,  0.1282,  0.0066,\n",
      "         -0.0011, -0.0177, -0.0375,  0.0091, -0.0519, -0.0265, -0.1124,  0.0392,\n",
      "         -0.0891,  0.0007, -0.1051, -0.0422,  0.0286, -0.1196,  0.0996,  0.0285,\n",
      "          0.0273, -0.1252,  0.0802,  0.1090, -0.1275,  0.0871,  0.0329,  0.0991,\n",
      "         -0.0364,  0.0376,  0.1191,  0.1033,  0.0099,  0.0382,  0.0133,  0.0605,\n",
      "          0.1116,  0.0153,  0.0468,  0.0369,  0.0216,  0.0708, -0.0703, -0.0282,\n",
      "          0.0909,  0.0882, -0.0437,  0.1092, -0.0853,  0.0786, -0.0747,  0.0501,\n",
      "          0.0156,  0.0733,  0.0980,  0.0898],\n",
      "        [-0.0606,  0.0970, -0.0551,  0.0845,  0.0216,  0.0394, -0.0617, -0.0097,\n",
      "          0.0116,  0.0793, -0.1104, -0.0606, -0.0330,  0.1266,  0.0534, -0.0014,\n",
      "          0.1014,  0.0004,  0.0357,  0.0953,  0.1092,  0.0292, -0.0188, -0.0166,\n",
      "         -0.0594, -0.0706, -0.0003, -0.1151,  0.1157, -0.0624, -0.0843, -0.0699,\n",
      "         -0.1158, -0.1104,  0.1249, -0.0534,  0.0851, -0.0122, -0.1281,  0.0744,\n",
      "         -0.0137,  0.0552, -0.0594,  0.0554, -0.1006, -0.1168,  0.1122, -0.0795,\n",
      "          0.0794,  0.0075,  0.0734, -0.0484, -0.0451, -0.0555,  0.1026, -0.0033,\n",
      "         -0.0413,  0.0127,  0.0285, -0.0505]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0227, -0.1175,  0.0680,  0.0912,  0.0614, -0.1225,  0.0593, -0.0946,\n",
      "        -0.1187,  0.0675], dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-6.9817e-03,  4.5777e-03, -4.9030e-03,  ..., -3.6128e-03,\n",
      "         -4.5097e-03, -8.6913e-03],\n",
      "        [ 5.3483e-03,  2.1009e-03, -4.0274e-03,  ...,  2.9794e-04,\n",
      "         -8.1730e-03,  3.3954e-03],\n",
      "        [ 5.4492e-03,  6.3525e-03, -3.0126e-03,  ...,  1.8130e-03,\n",
      "         -7.0947e-03,  6.0732e-03],\n",
      "        ...,\n",
      "        [-4.2957e-03, -7.2283e-05, -2.6162e-03,  ..., -1.9180e-05,\n",
      "          8.0758e-03, -6.8222e-03],\n",
      "        [ 8.9584e-04,  3.0305e-03, -2.9353e-03,  ..., -1.5818e-03,\n",
      "          6.1427e-03,  4.7564e-04],\n",
      "        [-1.9746e-03, -5.2473e-03,  9.3382e-05,  ...,  1.0062e-03,\n",
      "          6.0104e-03, -3.9062e-03]], dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 9.8902e-04, -5.0519e-03,  8.3779e-03, -4.4539e-04, -8.1417e-03,\n",
      "        -8.8369e-03,  2.8676e-03, -1.6763e-03,  5.9314e-03,  9.0811e-03,\n",
      "         2.8765e-03,  7.3653e-03, -5.4153e-03,  7.9555e-03,  7.4612e-03,\n",
      "         4.9858e-03,  6.8479e-03, -3.4705e-03, -3.9091e-03,  1.7949e-03,\n",
      "         3.0502e-03, -8.3913e-05,  7.9070e-03, -1.0870e-03, -8.1950e-03,\n",
      "        -5.3878e-04,  8.8807e-05,  1.4696e-03, -6.3402e-03,  1.1053e-03,\n",
      "         7.8108e-03,  6.6300e-03,  4.7477e-03,  4.3182e-03, -1.1336e-03,\n",
      "         3.6385e-03, -6.0294e-04,  9.1739e-03,  2.5272e-03, -4.3972e-04,\n",
      "        -9.4518e-03, -4.2349e-03, -9.3551e-03, -6.3790e-03,  7.3219e-03,\n",
      "         2.7303e-03,  8.8474e-03,  3.0466e-03,  2.3864e-03, -6.4867e-03,\n",
      "         4.7300e-03,  6.9632e-03, -1.1103e-03, -9.0723e-03, -5.5731e-03,\n",
      "         8.7759e-05, -6.9800e-03,  2.6526e-04,  3.0774e-03,  6.3944e-04,\n",
      "        -5.1755e-03, -9.1275e-03,  7.8337e-03,  8.0643e-03,  4.6233e-03,\n",
      "         1.8615e-03,  6.7560e-04, -8.3295e-03,  2.9317e-03,  6.7370e-03,\n",
      "         5.8109e-03, -2.1985e-03, -5.7466e-03,  2.6769e-04,  3.1003e-03,\n",
      "        -3.4976e-03,  6.5477e-03,  6.0350e-03,  2.5895e-03,  6.3246e-03,\n",
      "         9.3241e-04, -5.9513e-03, -8.6575e-05,  7.0952e-04,  4.4644e-03,\n",
      "        -8.1055e-04, -8.3430e-04, -7.6172e-03,  6.5365e-03, -6.9570e-03,\n",
      "         7.4505e-04,  7.7549e-03,  3.8468e-03, -7.7034e-03,  7.7971e-03,\n",
      "         3.0408e-03,  3.9788e-03, -3.0050e-03, -4.0788e-03, -5.2764e-04],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0106, -0.0050, -0.0115,  ...,  0.0047, -0.0081, -0.0123],\n",
      "        [ 0.0188,  0.0088, -0.0207,  ...,  0.0015,  0.0059, -0.0007],\n",
      "        [-0.0039,  0.0061,  0.0060,  ..., -0.0210,  0.0210,  0.0109],\n",
      "        ...,\n",
      "        [ 0.0222, -0.0024,  0.0044,  ...,  0.0166,  0.0025,  0.0209],\n",
      "        [-0.0120, -0.0177, -0.0045,  ..., -0.0189,  0.0119, -0.0053],\n",
      "        [-0.0190,  0.0156,  0.0061,  ..., -0.0204, -0.0143, -0.0187]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.0221e-02,  2.1412e-02,  2.6296e-04,  1.6627e-02,  1.9236e-02,\n",
      "        -9.2457e-03, -6.9336e-03,  1.7701e-02, -1.9206e-02,  4.0772e-03,\n",
      "        -1.7809e-02, -1.4714e-02, -1.9397e-02, -1.9858e-02, -1.4351e-02,\n",
      "         7.9034e-03, -1.3340e-02,  1.3871e-02, -1.1057e-02,  9.9716e-03,\n",
      "         3.2663e-03, -9.0993e-03,  1.0864e-02,  9.5739e-03,  5.4142e-03,\n",
      "        -1.4479e-02, -6.2019e-03, -9.8864e-03,  1.9687e-02,  5.4718e-03,\n",
      "         8.5501e-03,  4.7539e-03, -8.2210e-03,  7.7502e-03, -5.8051e-03,\n",
      "        -6.6393e-03, -1.5280e-02,  1.3268e-02,  1.5339e-02,  1.8959e-02,\n",
      "        -1.6201e-02,  9.4332e-03,  4.8432e-03, -1.3527e-02,  5.9941e-03,\n",
      "         1.5665e-02, -1.2534e-02,  7.4956e-03, -1.4735e-02, -1.6839e-02,\n",
      "         2.1613e-02, -2.1293e-02,  2.3202e-03,  1.5604e-02,  6.5883e-03,\n",
      "        -2.0861e-02,  7.3556e-03, -1.9514e-02,  1.4311e-02, -6.1964e-03,\n",
      "        -8.4510e-03, -4.6225e-03, -1.4424e-02, -1.6839e-02, -2.0872e-02,\n",
      "         1.7895e-02, -1.6849e-02, -9.9634e-03, -1.8269e-02, -4.9430e-03,\n",
      "        -1.3384e-02,  1.1965e-03,  6.7222e-03,  1.8535e-02, -6.7816e-03,\n",
      "        -8.5577e-03,  1.3124e-02, -3.3116e-03, -1.3951e-02, -1.3324e-02,\n",
      "        -8.2253e-03, -3.9655e-04,  4.3502e-03, -1.7729e-02,  1.0021e-02,\n",
      "        -1.1777e-02, -7.0275e-03,  5.3846e-03, -9.9569e-03,  1.2807e-04,\n",
      "         7.8679e-03, -1.6362e-02,  1.6972e-02,  1.2774e-02, -7.5317e-03,\n",
      "        -1.0573e-02, -2.1910e-03, -5.5886e-03,  2.1664e-02,  1.1978e-02,\n",
      "         5.3762e-03,  1.0141e-02,  1.9949e-02,  1.9324e-02, -1.2750e-02,\n",
      "         8.3204e-03, -2.0243e-02, -7.7148e-03, -2.0720e-02,  4.9702e-03,\n",
      "        -1.5683e-02, -9.1467e-03, -1.6070e-02, -9.7564e-03, -1.5646e-02,\n",
      "        -1.8123e-02,  2.3977e-04,  1.5603e-02,  7.3050e-03, -1.7251e-02,\n",
      "        -2.1456e-02, -2.1451e-02, -8.7691e-03,  1.1165e-03, -3.1918e-03,\n",
      "         1.5413e-02,  2.0558e-02,  2.1545e-02, -7.4951e-03, -1.5684e-02,\n",
      "         5.9209e-03, -1.3546e-02,  1.8425e-02,  1.8392e-02,  1.7075e-02,\n",
      "         1.1479e-02,  1.7639e-02, -1.9867e-02, -7.0024e-03,  3.7141e-03,\n",
      "         1.1871e-03,  1.0328e-02,  5.1997e-03, -3.0374e-03,  6.3311e-03,\n",
      "        -5.1905e-04,  1.6489e-02, -6.2457e-03, -1.4143e-02,  9.0274e-03,\n",
      "         1.9887e-02, -1.3249e-02, -1.3722e-02, -1.0601e-02,  3.2090e-03,\n",
      "         1.4689e-02, -2.0818e-02, -9.8089e-03,  4.3937e-03,  1.9346e-03,\n",
      "        -1.3048e-03,  1.5994e-03,  5.6026e-03, -1.9502e-02, -7.6516e-03,\n",
      "         2.4967e-04, -8.6154e-03,  8.7344e-03,  1.5210e-02, -2.2220e-02,\n",
      "        -5.7865e-04,  9.7022e-03, -6.7954e-03, -8.2752e-03, -1.3910e-02,\n",
      "         1.0651e-02, -1.4160e-02,  2.0361e-02,  9.5450e-03, -8.3595e-03,\n",
      "        -2.6745e-03, -1.2116e-02, -2.2928e-03,  1.1858e-03,  1.1969e-02,\n",
      "        -4.1535e-03,  4.5050e-03, -2.3186e-03,  9.5679e-03,  2.1167e-02,\n",
      "         1.0137e-02, -1.8129e-02,  1.3950e-02,  1.9714e-02,  2.6710e-03,\n",
      "         5.4027e-03, -1.4272e-02,  1.7528e-02,  1.5584e-03, -3.0367e-03,\n",
      "         5.7494e-03,  1.2569e-02, -4.6612e-03,  2.1324e-02, -6.5319e-03,\n",
      "        -1.4657e-02, -1.0714e-02,  7.7011e-03,  1.7121e-02, -7.3545e-04,\n",
      "         1.4851e-02, -1.4955e-02, -1.2469e-02,  1.5571e-02,  2.7190e-03,\n",
      "         6.0973e-04, -1.0831e-02, -2.1769e-02, -1.5876e-02,  3.0554e-03,\n",
      "        -1.3225e-02,  8.1735e-05,  1.3330e-02, -2.2613e-04,  1.7685e-02,\n",
      "        -1.5665e-02, -2.1179e-02,  2.9681e-03, -5.6166e-03, -5.1702e-03,\n",
      "        -5.0018e-03,  4.0449e-04,  9.2481e-03,  4.1951e-03,  1.2707e-02,\n",
      "         1.4231e-02, -1.0552e-02, -5.9738e-03,  1.7170e-02, -1.2734e-02,\n",
      "         1.2318e-02, -7.4865e-03, -1.8533e-02,  5.8380e-03, -7.6164e-03,\n",
      "         9.1129e-03,  1.0155e-02,  2.1258e-02,  1.1024e-03, -2.0631e-02,\n",
      "         6.2728e-03, -8.9161e-03,  9.3830e-03,  1.0336e-04,  1.0001e-03,\n",
      "        -1.3354e-02, -2.2224e-02, -1.0485e-02, -2.2133e-02,  1.6707e-02,\n",
      "         1.9722e-02, -2.0195e-02, -1.7156e-03,  7.3143e-03, -1.4286e-02,\n",
      "         1.8538e-02, -7.2258e-03, -1.6708e-02,  1.9849e-02,  1.5898e-02,\n",
      "        -2.0926e-02, -2.0453e-02, -7.0980e-03, -1.8355e-02,  1.1567e-02,\n",
      "         1.1703e-02, -1.9812e-02,  1.9884e-03, -5.1167e-03, -1.7874e-02,\n",
      "        -2.1000e-02, -1.7181e-02, -1.7519e-02,  3.6915e-03,  1.4084e-03,\n",
      "         1.3564e-02,  1.2555e-02,  2.8552e-03,  2.5782e-04, -8.6546e-03,\n",
      "        -1.2034e-02,  6.4005e-03,  1.5616e-02,  3.6167e-03,  1.2722e-02,\n",
      "         1.5982e-02, -1.3776e-02,  1.8964e-02, -1.2212e-02,  2.0599e-02,\n",
      "        -8.6646e-03,  8.0074e-03, -1.1961e-02, -9.7398e-03, -2.0082e-02,\n",
      "         5.8432e-03, -2.8869e-03,  2.1155e-02,  8.5952e-03,  1.6153e-02,\n",
      "         1.3196e-02,  1.7124e-02, -1.9965e-02,  7.8834e-03, -1.7545e-02,\n",
      "        -7.0839e-03, -2.0581e-02,  1.5813e-02,  2.3179e-03, -9.9522e-03,\n",
      "        -1.1712e-02, -1.5782e-02, -6.5442e-04,  1.5291e-02, -6.4617e-03,\n",
      "         1.3411e-03, -1.4218e-02,  1.9941e-02,  2.1435e-03,  4.7721e-03,\n",
      "        -7.1488e-03,  1.8006e-02,  1.9479e-02,  1.5061e-02, -7.3346e-03,\n",
      "         1.3912e-02,  1.9459e-02,  1.3209e-02, -2.0263e-02,  8.4617e-03,\n",
      "         2.0799e-02,  1.6363e-03,  1.9112e-02, -1.0716e-02,  5.4636e-03,\n",
      "         2.2045e-02,  5.3562e-03, -2.1516e-02, -8.3085e-03, -4.4659e-03,\n",
      "        -1.1003e-02, -1.7431e-02, -1.6114e-02, -1.5233e-02,  2.2081e-02,\n",
      "        -1.3808e-02,  1.1089e-02,  1.2563e-02, -1.9678e-02,  1.1201e-02,\n",
      "         1.1114e-02,  9.4134e-03, -1.8469e-03, -1.2822e-02,  1.0902e-02,\n",
      "        -2.9351e-03,  2.2246e-03, -4.4256e-03, -1.5687e-02, -1.7235e-02,\n",
      "        -1.2535e-02,  1.4102e-03, -7.6531e-04,  1.8585e-02,  1.8573e-02,\n",
      "         1.7047e-02, -6.6483e-03, -4.0380e-03,  1.9824e-02, -9.2905e-04,\n",
      "        -1.4829e-02, -8.5384e-03,  1.0632e-02, -1.4714e-02,  2.1999e-02,\n",
      "        -9.1837e-04,  1.0996e-02, -1.3792e-02, -1.0319e-02,  1.8852e-03,\n",
      "        -1.4723e-02, -1.1347e-06,  4.8779e-03, -7.8987e-03,  1.2821e-03,\n",
      "         1.5928e-03, -1.9185e-03,  5.7713e-03,  2.0361e-04, -2.0422e-02,\n",
      "         2.4669e-03,  1.5354e-02,  4.8379e-03, -1.9060e-02, -1.3484e-02,\n",
      "        -1.1557e-02, -9.1004e-04, -2.1877e-02,  1.8361e-02, -1.0471e-02,\n",
      "        -1.7253e-03, -7.6565e-03,  1.8654e-02,  1.6632e-02, -1.9979e-02,\n",
      "        -1.1454e-02,  1.1583e-02,  1.1759e-02,  1.5156e-02, -2.0615e-02,\n",
      "         1.6410e-02, -1.4855e-02,  5.4493e-03, -5.4569e-03,  7.7624e-03,\n",
      "        -1.9919e-02, -4.5701e-03,  1.7741e-02, -2.1549e-02,  1.0790e-02,\n",
      "         2.1824e-02,  1.3142e-02, -2.0754e-02,  1.9095e-02, -7.6861e-03,\n",
      "        -6.0717e-03, -7.5875e-04,  1.0044e-02,  2.0229e-02,  5.9634e-03,\n",
      "         3.9845e-03, -2.8175e-03, -1.2818e-02, -5.8966e-03,  1.3051e-02,\n",
      "        -1.3583e-02, -1.3454e-02,  9.3953e-03,  1.4437e-02,  7.9635e-03,\n",
      "        -6.5212e-03,  5.8491e-03, -2.7746e-03,  2.9506e-03,  2.4058e-04,\n",
      "        -1.1616e-02,  1.2192e-02,  2.1417e-03, -1.0270e-02,  1.7949e-02,\n",
      "         2.0212e-02, -1.5960e-02,  8.0002e-03,  6.1013e-03, -1.8580e-02,\n",
      "        -1.4076e-02,  8.3395e-03,  2.3336e-03, -1.1533e-03, -8.4387e-03,\n",
      "        -1.2751e-02,  2.0648e-03,  5.7830e-03,  1.2726e-02,  4.2764e-03,\n",
      "         1.4156e-02, -1.3304e-02,  1.0927e-02, -1.1101e-03,  8.7774e-03,\n",
      "        -1.8727e-02,  1.4698e-02,  1.6982e-02,  1.2301e-02, -5.8836e-03,\n",
      "        -1.6853e-02,  1.2000e-02,  1.6697e-02, -1.1884e-02,  1.4140e-02,\n",
      "        -3.5282e-03,  8.2882e-03, -6.9860e-03, -1.8932e-02,  7.4971e-03,\n",
      "        -5.9955e-03,  8.5623e-03, -1.5192e-02, -2.0380e-02,  4.1231e-03],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0269, -0.0409,  0.0347,  ...,  0.0046, -0.0318,  0.0240],\n",
      "        [-0.0062, -0.0164, -0.0285,  ...,  0.0430,  0.0286,  0.0408],\n",
      "        [ 0.0123,  0.0072, -0.0223,  ..., -0.0430,  0.0094,  0.0040],\n",
      "        ...,\n",
      "        [-0.0008, -0.0249,  0.0388,  ...,  0.0391,  0.0238,  0.0401],\n",
      "        [ 0.0356,  0.0176, -0.0387,  ...,  0.0210, -0.0176, -0.0204],\n",
      "        [ 0.0308,  0.0429,  0.0407,  ..., -0.0175, -0.0330, -0.0284]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0152, -0.0369, -0.0199,  0.0298,  0.0366, -0.0037, -0.0266, -0.0150,\n",
      "         0.0442, -0.0182,  0.0097,  0.0321,  0.0141,  0.0229, -0.0380, -0.0231,\n",
      "         0.0094,  0.0317, -0.0112,  0.0389,  0.0378, -0.0181, -0.0407,  0.0259,\n",
      "        -0.0164,  0.0260,  0.0191, -0.0362,  0.0394,  0.0365,  0.0316,  0.0046,\n",
      "         0.0151, -0.0198, -0.0211, -0.0336,  0.0271, -0.0155,  0.0232,  0.0142,\n",
      "        -0.0180,  0.0289, -0.0322, -0.0306, -0.0350, -0.0348,  0.0219,  0.0001,\n",
      "         0.0428, -0.0079, -0.0363, -0.0296,  0.0339,  0.0002,  0.0005, -0.0351,\n",
      "         0.0294, -0.0120,  0.0423, -0.0422,  0.0418,  0.0185, -0.0139, -0.0035,\n",
      "        -0.0389, -0.0080, -0.0039,  0.0282,  0.0266,  0.0250,  0.0358,  0.0422,\n",
      "        -0.0247, -0.0122, -0.0159, -0.0360, -0.0407,  0.0085, -0.0184,  0.0108,\n",
      "         0.0197, -0.0192, -0.0302, -0.0112,  0.0383,  0.0415, -0.0146,  0.0193,\n",
      "         0.0313,  0.0029, -0.0185, -0.0356,  0.0387,  0.0307,  0.0404,  0.0362,\n",
      "        -0.0293,  0.0026, -0.0221, -0.0103], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0291, -0.0733, -0.0547,  0.0966,  0.0468,  0.0704, -0.0768,  0.0886,\n",
      "          0.0062,  0.0834,  0.0134,  0.0293,  0.0976, -0.0803,  0.0353, -0.0747,\n",
      "         -0.0652,  0.0068, -0.0118, -0.0909,  0.0843,  0.0060,  0.0093,  0.0184,\n",
      "          0.0155, -0.0880, -0.0469,  0.0724, -0.0495, -0.0111,  0.0578,  0.0215,\n",
      "         -0.0820,  0.0632,  0.0422, -0.0070, -0.0787, -0.0960,  0.0098, -0.0847,\n",
      "          0.0046,  0.0477, -0.0836,  0.0965, -0.0334, -0.0405, -0.0850, -0.0422,\n",
      "         -0.0836,  0.0720, -0.0273,  0.0009, -0.0595,  0.0200, -0.0194,  0.0504,\n",
      "         -0.0436, -0.0448,  0.0215, -0.0536,  0.0458, -0.0773, -0.0662, -0.0965,\n",
      "          0.0899,  0.0511, -0.0808,  0.0398,  0.0365, -0.0604,  0.0485,  0.0614,\n",
      "         -0.0283, -0.0292, -0.0576,  0.0114, -0.0588, -0.0103,  0.0085, -0.0724,\n",
      "         -0.0853,  0.0039,  0.0099,  0.0305,  0.0439,  0.0552, -0.0646, -0.0532,\n",
      "          0.0226, -0.0273, -0.0706, -0.0606, -0.0301,  0.0778, -0.0641, -0.0693,\n",
      "          0.0601,  0.0369,  0.0550, -0.0810]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0810], dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0578, -0.0898, -0.0810, -0.0998, -0.0853,  0.0850,  0.0077,  0.0880,\n",
      "         -0.0098, -0.0277, -0.0315,  0.0697, -0.0065, -0.0050,  0.0289, -0.0502,\n",
      "          0.0882,  0.0324,  0.0799,  0.0307,  0.0791,  0.0528,  0.0660, -0.0490,\n",
      "          0.0141,  0.0555, -0.0437,  0.0591, -0.0973,  0.0530, -0.0916, -0.0607,\n",
      "          0.0709,  0.0896, -0.0764, -0.0299, -0.0648,  0.0011,  0.0745,  0.0515,\n",
      "          0.0106, -0.0763, -0.0600,  0.0071,  0.0950, -0.0397, -0.0968,  0.0166,\n",
      "          0.0656,  0.0917,  0.0772, -0.0076,  0.0198,  0.0932,  0.0433,  0.0808,\n",
      "         -0.0991,  0.0735, -0.0357,  0.0550,  0.0943, -0.0035, -0.0395, -0.0781,\n",
      "          0.0360, -0.0963, -0.0310,  0.0681, -0.0181, -0.0560,  0.0052,  0.0505,\n",
      "          0.0271, -0.0659,  0.0861, -0.0583, -0.0484, -0.0863,  0.0965, -0.0933,\n",
      "         -0.0547,  0.0873,  0.0691,  0.0472, -0.0402, -0.0548,  0.0091,  0.0471,\n",
      "          0.0237, -0.0465, -0.0416,  0.0299, -0.0172,  0.0567, -0.0385,  0.0616,\n",
      "         -0.0081,  0.0881,  0.0926, -0.0735]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0453], dtype=torch.float64, requires_grad=True)\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model\n",
    "with open('model.pkl','wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "\n",
    "with open('model.pkl','rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    print(model)\n",
    "\n",
    "# save model parameters\n",
    "torch.save(model.state_dict(),'model_params.pkl')\n",
    "\n",
    "with open('model_params.pkl','rb') as f:\n",
    "    model.load_state_dict(torch.load(f))\n",
    "    print(model)\n",
    "weights = []\n",
    "j = 0\n",
    "for i in model.parameters():\n",
    "    weights.append(i)\n",
    "    print(i)\n",
    "    j += 1\n",
    "print(j)\n",
    "\n",
    "with open('weights.pkl','wb') as f:\n",
    "    pickle.dump(weights,f)\n",
    "\n",
    "# print(len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([344, 642]), tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]], dtype=torch.float64), tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]], dtype=torch.float64), tensor([22, 32]), tensor([1, 1])]\n",
      "dx: torch.Size([2, 2012])\n",
      "sc torch.Size([2, 60])\n",
      "sx torch.Size([2, 10060])\n",
      "dc torch.Size([4, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15910/364648262.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dense = torch.tensor(data[1])\n",
      "/tmp/ipykernel_15910/364648262.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sparse = torch.tensor(data[2])\n",
      "/tmp/ipykernel_15910/364648262.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dx = torch.tensor(dx)\n",
      "/tmp/ipykernel_15910/364648262.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sx = torch.tensor(sx)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 4 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [388], line 32\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdc\u001b[39m\u001b[39m\"\u001b[39m,dc\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m \u001b[39m# dc = Model.dense_c(sc)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[39m# s1 = generate_s1(s1_len)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# concatanete dc and dx to get d\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((dc,dx),\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     34\u001b[0m s \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((sx,sc),\u001b[39m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m,d\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "data = None\n",
    "for i in train_batches:\n",
    "    data = i\n",
    "print(data)\n",
    "\n",
    "dense = torch.tensor(data[1])\n",
    "sparse = torch.tensor(data[2])\n",
    "config = data[3]\n",
    "label = data[4]\n",
    "\n",
    "\n",
    "dx = dense\n",
    "sx = sparse\n",
    "\n",
    "sc = np.eye(60)[config]\n",
    "dx = torch.tensor(dx)\n",
    "\n",
    "\n",
    "# sc = np.eye(60)[sc]\n",
    "sc = torch.tensor(sc)\n",
    "sx = torch.tensor(sx)\n",
    "dc = torch.randn((4,20))\n",
    "print(\"dx:\",dx.shape)\n",
    "print(\"sc\",sc.shape)\n",
    "print(\"sx\",sx.shape)\n",
    "print(\"dc\",dc.shape)\n",
    "# dc = Model.dense_c(sc)\n",
    "\n",
    "# s1 = generate_s1(s1_len)\n",
    "# concatanete dc and dx to get d\n",
    "d = torch.cat((dc,dx),1)\n",
    "\n",
    "s = torch.cat((sx,sc),1)\n",
    "print(\"d\",d.shape)\n",
    "print(\"s\",s.shape)\n",
    "# s1 = torch.tens(s1)\n",
    "# wide_s = torch.cat((s,s1),1)\n",
    "# wide_s = torch.tensor(wide_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
